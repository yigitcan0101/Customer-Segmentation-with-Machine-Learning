# src/data/ml_auto_segmentation_engine.py

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')


class MLAutoSegmentationEngine:
    """
    ğŸ§  MACHINE LEARNING POWERED AUTO SEGMENTATION ENGINE

    Ham CSV verilerinden otomatik pattern discovery:
    1. Description â†’ Automatic Product Category Discovery
    2. Country â†’ Automatic Geographic Clustering
    3. Pattern-based Customer Behavior Segmentation
    4. Integration with existing neural network
    """

    def __init__(self):
        self.raw_data = None
        self.rfm_data = None
        self.merged_data = None

        # ML Models
        self.product_clustering_model = None
        self.geographic_clustering_model = None
        self.tfidf_vectorizer = None
        self.scaler = StandardScaler()

        # Discovered patterns
        self.discovered_product_categories = {}
        self.discovered_geographic_segments = {}
        self.discovered_patterns = {}

        print("ğŸ§  ML-POWERED AUTO SEGMENTATION ENGINE")
        print("ğŸ¤– Derin Makine Ã–ÄŸrenmesi ile Otomatik Pattern Discovery")
        print("=" * 70)

    def load_and_prepare_data(self, raw_csv_path="data/processed/online_retail_data.csv",
                              rfm_csv_path="data/processed/rfm_analysis_results.csv"):
        """
        Step 1: Veriyi yÃ¼kle ve ML iÃ§in hazÄ±rla
        """
        print("ğŸ“– Veri yÃ¼kleme ve ML preprocessing...")

        # Load datasets
        self.raw_data = pd.read_csv(raw_csv_path)
        self.rfm_data = pd.read_csv(rfm_csv_path)

        # Data preprocessing
        self.raw_data['InvoiceDate'] = pd.to_datetime(self.raw_data['InvoiceDate'])
        self.raw_data['TotalAmount'] = self.raw_data['Price'] * self.raw_data['Quantity']

        # Clean descriptions
        self.raw_data['Description_Clean'] = self.raw_data['Description'].apply(self._clean_text)

        # Merge with RFM
        self.merged_data = self.raw_data.merge(
            self.rfm_data[['Customer ID', 'Segment', 'CustomerValue', 'R_Score', 'F_Score', 'M_Score']],
            on='Customer ID',
            how='left'
        ).dropna(subset=['Segment'])

        print(f"âœ… Data loaded: {len(self.merged_data):,} transactions")
        print(f"ğŸ“Š Countries: {self.merged_data['Country'].nunique()}")
        print(f"ğŸ·ï¸ Unique products: {self.merged_data['Description'].nunique():,}")

        return self.merged_data

    def auto_discover_product_categories(self, n_categories=10):
        """
        ğŸ·ï¸ AUTOMATIC PRODUCT CATEGORY DISCOVERY
        TF-IDF + K-Means ile Ã¼rÃ¼n kategorilerini otomatik keÅŸfet
        """
        print(f"\nğŸ¤– AUTOMATIC PRODUCT CATEGORY DISCOVERY")
        print("=" * 50)

        # Get unique product descriptions
        unique_descriptions = self.merged_data['Description_Clean'].dropna().unique()
        print(f"ğŸ“¦ Analyzing {len(unique_descriptions):,} unique products...")

        # TF-IDF Vectorization
        print("ğŸ” TF-IDF vectorization...")
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )

        tfidf_matrix = self.tfidf_vectorizer.fit_transform(unique_descriptions)

        # K-Means Clustering
        print(f"ğŸ§  K-Means clustering into {n_categories} categories...")
        self.product_clustering_model = KMeans(n_clusters=n_categories, random_state=42, n_init=10)
        product_clusters = self.product_clustering_model.fit_predict(tfidf_matrix)

        # Analyze discovered categories
        product_df = pd.DataFrame({
            'Description': unique_descriptions,
            'Category_ID': product_clusters
        })

        # Get top keywords for each category
        feature_names = self.tfidf_vectorizer.get_feature_names_out()

        discovered_categories = {}
        for category_id in range(n_categories):
            # Get cluster center
            cluster_center = self.product_clustering_model.cluster_centers_[category_id]

            # Get top features for this cluster
            top_indices = cluster_center.argsort()[-10:][::-1]
            top_keywords = [feature_names[i] for i in top_indices]

            # Get products in this category
            category_products = product_df[product_df['Category_ID'] == category_id]['Description'].tolist()

            # Assign category name based on top keywords
            category_name = self._assign_category_name(top_keywords, category_products)

            discovered_categories[category_id] = {
                'name': category_name,
                'keywords': top_keywords,
                'product_count': len(category_products),
                'sample_products': category_products[:5]
            }

            print(f"ğŸ·ï¸ Category {category_id}: {category_name}")
            print(f"   ğŸ“¦ Products: {len(category_products)}")
            print(f"   ğŸ”‘ Keywords: {', '.join(top_keywords[:5])}")
            print(f"   ğŸ“ Sample: {category_products[0] if category_products else 'N/A'}")

        self.discovered_product_categories = discovered_categories

        # Add category to merged data
        product_category_map = dict(zip(product_df['Description'], product_df['Category_ID']))
        self.merged_data['Auto_Product_Category'] = self.merged_data['Description_Clean'].map(product_category_map)

        print(f"\nâœ… Discovered {len(discovered_categories)} product categories automatically!")
        return discovered_categories

    def auto_discover_geographic_segments(self, min_country_size=20):
        """
        ğŸŒ AUTOMATIC GEOGRAPHIC SEGMENTATION
        GERÃ‡EK CSV verisinden country behavior patterns ile otomatik segmentasyon
        """
        print(f"\nğŸŒ AUTOMATIC GEOGRAPHIC SEGMENTATION")
        print("=" * 50)

        # Ã–NCE: GerÃ§ek country distribution'Ä± analiz et
        country_distribution = self.merged_data['Country'].value_counts()
        print(f"ğŸ“Š GERÃ‡EK COUNTRY DISTRIBUTION:")
        for country, count in country_distribution.head(10).items():
            print(f"   {country}: {count:,} transactions")

        total_countries = len(country_distribution)
        print(f"ğŸŒ Toplam Ã¼lke sayÄ±sÄ±: {total_countries}")

        # Country-level feature aggregation (GERÃ‡EK VERÄ°)
        country_features = self.merged_data.groupby('Country').agg({
            'TotalAmount': ['sum', 'mean', 'std'],
            'Quantity': ['sum', 'mean'],
            'Price': ['mean', 'std'],
            'Customer ID': 'nunique',
            'Invoice': 'nunique'
        }).round(2)

        country_features.columns = ['Total_Revenue', 'Avg_Order_Value', 'Revenue_Std',
                                    'Total_Quantity', 'Avg_Quantity', 'Avg_Price', 'Price_Std',
                                    'Unique_Customers', 'Total_Orders']

        # Minimum size filter (gerÃ§ek data'ya gÃ¶re adjust)
        print(f"\nğŸ” Filtering countries with min {min_country_size} customers...")
        before_filter = len(country_features)
        country_features = country_features[country_features['Unique_Customers'] >= min_country_size]
        after_filter = len(country_features)

        print(f"ğŸ“Š Country filter: {before_filter} â†’ {after_filter} countries")
        print(f"ğŸ—ºï¸ CLUSTERING COUNTRIES:")
        for country in country_features.index:
            customers = country_features.loc[country, 'Unique_Customers']
            avg_order = country_features.loc[country, 'Avg_Order_Value']
            print(f"   {country}: {customers:,} customers, Â£{avg_order:.2f} avg order")

        # EÄŸer Ã¼lke sayÄ±sÄ± Ã§ok azsa, manual segmentation yap
        if len(country_features) <= 3:
            print("âš ï¸ Too few countries for clustering, using rule-based segmentation...")
            return self._manual_geographic_segmentation(country_features)

        # Feature engineering for clustering
        clustering_features = ['Avg_Order_Value', 'Avg_Quantity', 'Avg_Price',
                               'Unique_Customers', 'Total_Orders']

        # Handle missing values (std might be NaN for single-transaction countries)
        country_features_clean = country_features[clustering_features].fillna(0)

        # Standardize features
        X_geo = self.scaler.fit_transform(country_features_clean)

        # Determine optimal clusters (real data based)
        max_clusters = min(6, len(country_features_clean) - 1)
        optimal_clusters = self._find_optimal_clusters(X_geo, max_k=max_clusters)

        print(f"ğŸ§  Optimal clusters determined: {optimal_clusters}")

        # K-Means clustering
        self.geographic_clustering_model = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)
        geo_clusters = self.geographic_clustering_model.fit_predict(X_geo)

        # Analyze REAL geographic segments
        country_features['Geo_Segment'] = geo_clusters

        geographic_segments = {}
        for segment_id in range(optimal_clusters):
            segment_countries = country_features[country_features['Geo_Segment'] == segment_id]

            # GERÃ‡EK VERÄ° bazlÄ± segment naming
            segment_name = self._assign_geographic_segment_name_real(segment_countries, segment_id)

            geographic_segments[segment_id] = {
                'name': segment_name,
                'countries': segment_countries.index.tolist(),
                'country_count': len(segment_countries),
                'avg_order_value': segment_countries['Avg_Order_Value'].mean(),
                'total_customers': segment_countries['Unique_Customers'].sum(),
                'total_revenue': segment_countries['Total_Revenue'].sum(),
                'characteristics': self._describe_geographic_segment_real(segment_countries)
            }

            print(f"\nğŸŒ DISCOVERED Segment {segment_id}: {segment_name}")
            print(f"   ğŸ—ºï¸ Countries: {segment_countries.index.tolist()}")
            print(f"   ğŸ‘¥ Total customers: {segment_countries['Unique_Customers'].sum():,}")
            print(f"   ğŸ’° Avg order value: Â£{segment_countries['Avg_Order_Value'].mean():.2f}")
            print(f"   ğŸ’µ Total revenue: Â£{segment_countries['Total_Revenue'].sum():,.2f}")

        self.discovered_geographic_segments = geographic_segments

        # Add geographic segment to data
        country_to_segment = dict(zip(country_features.index, country_features['Geo_Segment']))
        self.merged_data['Auto_Geographic_Segment'] = self.merged_data['Country'].map(country_to_segment)

        print(f"\nâœ… Discovered {len(geographic_segments)} geographic segments from REAL data!")
        return geographic_segments

    def discover_customer_behavior_patterns(self):
        """
        ğŸ‘¤ CUSTOMER BEHAVIOR PATTERN DISCOVERY
        RFM + Auto features ile customer behavior patterns keÅŸfet
        """
        print(f"\nğŸ‘¤ CUSTOMER BEHAVIOR PATTERN DISCOVERY")
        print("=" * 50)

        # Customer-level feature engineering
        customer_features = self.merged_data.groupby('Customer ID').agg({
            'TotalAmount': ['sum', 'mean', 'std'],
            'Quantity': ['sum', 'mean', 'max'],
            'Price': ['mean', 'std', 'max'],
            'Invoice': 'nunique',
            'Auto_Product_Category': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else -1,
            'Auto_Geographic_Segment': 'first',
            'InvoiceDate': ['min', 'max']
        }).round(2)

        customer_features.columns = ['Total_Spend', 'Avg_Order', 'Order_Std',
                                     'Total_Items', 'Avg_Items', 'Max_Items',
                                     'Avg_Price', 'Price_Std', 'Max_Price',
                                     'Order_Frequency', 'Dominant_Product_Cat',
                                     'Geographic_Segment', 'First_Purchase', 'Last_Purchase']

        # Add RFM data
        customer_features = customer_features.merge(
            self.rfm_data[['Customer ID', 'Recency', 'Frequency', 'Monetary', 'Segment']],
            left_index=True, right_on='Customer ID'
        ).set_index('Customer ID')

        # Time-based features
        customer_features['Purchase_Span_Days'] = (
                pd.to_datetime(customer_features['Last_Purchase']) -
                pd.to_datetime(customer_features['First_Purchase'])
        ).dt.days

        # Discover behavior patterns with clustering
        behavior_features = ['Total_Spend', 'Order_Frequency', 'Avg_Order', 'Avg_Items',
                             'Purchase_Span_Days', 'Recency', 'Dominant_Product_Cat']

        # Handle missing values and prepare for clustering
        X_behavior = customer_features[behavior_features].fillna(0)
        X_behavior_scaled = StandardScaler().fit_transform(X_behavior)

        # Find behavior patterns
        optimal_behavior_clusters = self._find_optimal_clusters(X_behavior_scaled, max_k=8)
        behavior_kmeans = KMeans(n_clusters=optimal_behavior_clusters, random_state=42, n_init=10)
        behavior_clusters = behavior_kmeans.fit_predict(X_behavior_scaled)

        customer_features['Behavior_Pattern'] = behavior_clusters

        # Analyze behavior patterns
        behavior_patterns = {}
        for pattern_id in range(optimal_behavior_clusters):
            pattern_customers = customer_features[customer_features['Behavior_Pattern'] == pattern_id]

            pattern_name = self._assign_behavior_pattern_name(pattern_customers, pattern_id)

            behavior_patterns[pattern_id] = {
                'name': pattern_name,
                'customer_count': len(pattern_customers),
                'avg_total_spend': pattern_customers['Total_Spend'].mean(),
                'avg_frequency': pattern_customers['Order_Frequency'].mean(),
                'avg_recency': pattern_customers['Recency'].mean(),
                'dominant_segments': pattern_customers['Segment'].value_counts().head(3).to_dict()
            }

            print(f"ğŸ‘¤ Pattern {pattern_id}: {pattern_name}")
            print(f"   ğŸ‘¥ Customers: {len(pattern_customers):,}")
            print(f"   ğŸ’° Avg spend: Â£{pattern_customers['Total_Spend'].mean():.2f}")
            print(f"   ğŸ“Š Avg frequency: {pattern_customers['Order_Frequency'].mean():.1f}")

        self.discovered_patterns['behavior_patterns'] = behavior_patterns
        self.discovered_patterns['customer_features'] = customer_features

        print(f"\nâœ… Discovered {len(behavior_patterns)} behavior patterns automatically!")
        return behavior_patterns

    def generate_ml_enhanced_features(self):
        """
        ğŸ”§ ML ENHANCED FEATURES GENERATION
        KeÅŸfedilen pattern'larÄ± feature'lara Ã§evir
        """
        print(f"\nğŸ”§ ML ENHANCED FEATURES GENERATION")
        print("=" * 50)

        # Customer-level enhanced features
        enhanced_features = []

        customer_behavior = self.discovered_patterns['customer_features']

        for customer_id in self.rfm_data['Customer ID']:
            if customer_id not in customer_behavior.index:
                continue

            customer_data = customer_behavior.loc[customer_id]

            features = {
                'Customer ID': customer_id,

                # Auto-discovered Product Category Intelligence
                'auto_product_category_id': customer_data['Dominant_Product_Cat'],
                'product_category_name': self._get_category_name(customer_data['Dominant_Product_Cat']),

                # Auto-discovered Geographic Intelligence
                'auto_geographic_segment_id': customer_data['Geographic_Segment'],
                'geographic_segment_name': self._get_geographic_name(customer_data['Geographic_Segment']),

                # Auto-discovered Behavior Pattern
                'behavior_pattern_id': customer_data['Behavior_Pattern'],
                'behavior_pattern_name': self._get_behavior_name(customer_data['Behavior_Pattern']),

                # Enhanced metrics
                'purchase_span_days': customer_data['Purchase_Span_Days'],
                'order_value_consistency': 1 / (customer_data['Order_Std'] + 1),  # Lower std = more consistent
                'price_tier_preference': self._categorize_price_tier(customer_data['Avg_Price']),
                'bulk_buying_tendency': min(customer_data['Max_Items'] / customer_data['Avg_Items'], 5),

                # Cross-pattern features
                'segment_pattern_alignment': self._calculate_segment_pattern_alignment(customer_data),
                'geographic_behavior_score': self._calculate_geographic_behavior_score(customer_data)
            }

            enhanced_features.append(features)

        enhanced_df = pd.DataFrame(enhanced_features)

        # Merge with original RFM
        final_enhanced_dataset = self.rfm_data.merge(enhanced_df, on='Customer ID', how='left')

        # Handle missing values
        final_enhanced_dataset = self._handle_enhanced_missing_values(final_enhanced_dataset)

        print(f"âœ… Generated ML-enhanced features!")
        print(f"ğŸ“Š Original features: {len(self.rfm_data.columns)}")
        print(f"ğŸ“ˆ Enhanced features: {len(final_enhanced_dataset.columns)}")

        # Save enhanced dataset
        output_path = "data/processed/ml_enhanced_rfm_dataset.csv"
        final_enhanced_dataset.to_csv(output_path, index=False)
        print(f"ğŸ’¾ ML-enhanced dataset saved: {output_path}")

        return final_enhanced_dataset

    def create_neural_network_features(self):
        """
        ğŸ§  NEURAL NETWORK FEATURES CREATION
        Mevcut neural network iÃ§in optimize edilmiÅŸ feature set
        """
        print(f"\nğŸ§  NEURAL NETWORK FEATURES CREATION")
        print("=" * 50)

        # Generate enhanced features
        enhanced_dataset = self.generate_ml_enhanced_features()

        # Select and prepare features for neural network
        neural_features = [
            # Original RFM features
            'Recency', 'Frequency', 'Monetary', 'AvgOrderValue', 'CustomerValue',

            # ML-discovered features
            'auto_product_category_id', 'auto_geographic_segment_id', 'behavior_pattern_id',
            'purchase_span_days', 'order_value_consistency', 'price_tier_preference',
            'bulk_buying_tendency', 'segment_pattern_alignment', 'geographic_behavior_score'
        ]

        # Prepare for neural network (numerical only)
        neural_ready_data = enhanced_dataset[['Customer ID', 'Segment'] + neural_features].copy()

        # Handle any remaining missing values
        for col in neural_features:
            if col in neural_ready_data.columns:
                neural_ready_data[col].fillna(neural_ready_data[col].median(), inplace=True)

        # Create neural network input arrays
        X_neural = neural_ready_data[neural_features].values

        # Encode segments for neural network
        segment_encoder = LabelEncoder()
        y_neural = segment_encoder.fit_transform(neural_ready_data['Segment'])

        # Save for neural network integration
        np.save("data/processed/X_ml_enhanced_features.npy", X_neural)
        np.save("data/processed/y_ml_enhanced_labels.npy", y_neural)

        # Save feature names and segment mapping
        feature_info = {
            'feature_names': neural_features,
            'segment_mapping': dict(zip(segment_encoder.classes_, range(len(segment_encoder.classes_))))
        }

        import json
        with open("data/processed/ml_enhanced_feature_info.json", "w") as f:
            json.dump(feature_info, f)

        print(f"âœ… Neural network features created!")
        print(f"ğŸ“Š Input features: {X_neural.shape[1]} (was 5, now {X_neural.shape[1]})")
        print(f"ğŸ·ï¸ Output classes: {len(segment_encoder.classes_)}")
        print(f"ğŸ‘¥ Customers: {X_neural.shape[0]:,}")

        print(f"\nğŸ§  ENHANCED FEATURE LIST:")
        for i, feature in enumerate(neural_features):
            print(f"  {i + 1:2d}. {feature}")

        return X_neural, y_neural, neural_features, feature_info

    def run_complete_ml_discovery(self):
        """
        ğŸš€ COMPLETE ML-POWERED DISCOVERY PIPELINE
        """
        print("ğŸš€ COMPLETE ML-POWERED DISCOVERY PIPELINE")
        print("=" * 70)

        # Step 1: Load and prepare data
        self.load_and_prepare_data()

        # Step 2: Auto-discover product categories
        product_categories = self.auto_discover_product_categories(n_categories=10)

        # Step 3: Auto-discover geographic segments
        geographic_segments = self.auto_discover_geographic_segments(min_country_size=30)

        # Step 4: Discover customer behavior patterns
        behavior_patterns = self.discover_customer_behavior_patterns()

        # Step 5: Generate ML-enhanced features
        enhanced_dataset = self.generate_ml_enhanced_features()

        # Step 6: Create neural network ready features
        X_neural, y_neural, features, feature_info = self.create_neural_network_features()

        print(f"\nğŸ‰ COMPLETE ML DISCOVERY FINISHED!")
        print(f"ğŸ“Š GERÃ‡EK VERÄ° BAZLI RESULTS:")
        print(
            f"ğŸ·ï¸ Auto-discovered product categories: {len(product_categories)} (from your actual product descriptions)")
        print(f"ğŸŒ Auto-discovered geographic segments: {len(geographic_segments)} (from your actual countries)")
        print(f"ğŸ‘¤ Auto-discovered behavior patterns: {len(behavior_patterns)} (from your actual customer behavior)")
        print(f"ğŸ§  Neural network features: {len(features)} (enhanced from 5 â†’ {len(features)})")
        print(f"ğŸ¯ All insights based on YOUR ACTUAL CSV DATA - no placeholder examples!")

        return {
            'product_categories': product_categories,
            'geographic_segments': geographic_segments,
            'behavior_patterns': behavior_patterns,
            'enhanced_dataset': enhanced_dataset,
            'neural_features': (X_neural, y_neural, features, feature_info)
        }

    # HELPER METHODS
    def _clean_text(self, text):
        """Clean and normalize text"""
        if pd.isna(text):
            return ""
        text = str(text).upper()
        text = re.sub(r'[^A-Z0-9\s]', ' ', text)
        text = ' '.join(text.split())
        return text

    def _find_optimal_clusters(self, X, max_k=10):
        """Find optimal number of clusters using elbow method"""
        if len(X) < max_k:
            return min(3, len(X))

        inertias = []
        K_range = range(2, min(max_k + 1, len(X)))

        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X)
            inertias.append(kmeans.inertia_)

        # Simple elbow detection
        if len(inertias) >= 3:
            diffs = np.diff(inertias)
            second_diffs = np.diff(diffs)
            if len(second_diffs) > 0:
                elbow_idx = np.argmax(second_diffs) + 2
                return K_range[min(elbow_idx, len(K_range) - 1)]

        return min(5, len(K_range))

    def _assign_category_name(self, keywords, products):
        """
        GERÃ‡EK VERÄ° bazlÄ± category naming (not hardcoded)
        Machine learning discovered keywords'den intelligent naming
        """
        keyword_str = ' '.join(keywords).upper()

        # Analyze actual discovered keywords from your data
        if any(word in keyword_str for word in ['CHRISTMAS', 'ADVENT', 'FESTIVE', 'HOLIDAY', 'XMAS']):
            return 'Christmas_Seasonal'
        elif any(word in keyword_str for word in ['HEART', 'LOVE', 'VALENTINE', 'SWEETHEART']):
            return 'Heart_Romance_Theme'
        elif any(word in keyword_str for word in ['MUG', 'CUP', 'TEA', 'BOWL', 'PLATE', 'BAKING']):
            return 'Kitchen_Tableware'
        elif any(word in keyword_str for word in ['DOOR', 'MAT', 'DECORATION', 'ORNAMENT', 'WALL']):
            return 'Home_Decoration'
        elif any(word in keyword_str for word in ['CRAFT', 'FELT', 'DOLL', 'SEWING']):
            return 'Craft_Hobby'
        elif any(word in keyword_str for word in ['VINTAGE', 'RETRO', 'ANTIQUE', 'VICTORIAN']):
            return 'Vintage_Collectibles'
        elif any(word in keyword_str for word in ['LIGHT', 'CANDLE', 'HOLDER', 'LAMP']):
            return 'Lighting_Candles'
        elif any(word in keyword_str for word in ['BAG', 'BOX', 'STORAGE', 'BASKET']):
            return 'Storage_Organization'
        elif any(word in keyword_str for word in ['GLASS', 'CERAMIC', 'METAL', 'WOOD']):
            return 'Material_Goods'
        elif any(word in keyword_str for word in ['BOTTLE', 'WATER', 'HOT']):
            return 'Hot_Water_Bottles'
        elif any(word in keyword_str for word in ['FRAME', 'PICTURE', 'PHOTO']):
            return 'Picture_Frames'
        elif any(word in keyword_str for word in ['GARDEN', 'FLOWER', 'PLANT']):
            return 'Garden_Outdoor'
        else:
            # Use the most frequent keyword as category name
            if keywords:
                dominant_keyword = keywords[0].replace('_', ' ').title()
                return f'Auto_Category_{dominant_keyword}'
            else:
                return f'Discovered_Category_Unknown'

    def _assign_geographic_segment_name_real(self, segment_countries, segment_id):
        """
        GERÃ‡EK VERÄ° bazlÄ± geographic segment naming
        """
        countries = segment_countries.index.tolist()
        avg_order_value = segment_countries['Avg_Order_Value'].mean()
        total_customers = segment_countries['Unique_Customers'].sum()

        # UK Analysis
        if 'United Kingdom' in countries:
            if len(countries) == 1:
                return 'UK_Domestic_Market'
            elif len(countries) <= 3:
                other_countries = [c for c in countries if c != 'United Kingdom']
                return f'UK_Plus_{len(other_countries)}_Markets'
            else:
                return 'UK_International_Mix'

        # European Analysis
        eu_countries = [c for c in countries if c in ['Germany', 'France', 'Netherlands', 'Belgium', 'Austria', 'EIRE']]
        if len(eu_countries) >= 2:
            if avg_order_value > 100:
                return 'EU_Premium_Markets'
            else:
                return 'EU_Standard_Markets'

        # English Speaking Analysis
        english_countries = [c for c in countries if c in ['USA', 'Australia']]
        if len(english_countries) >= 1:
            return 'English_Speaking_International'

        # Single large market
        if len(countries) == 1:
            country_name = countries[0].replace(' ', '_')
            if total_customers > 100:
                return f'{country_name}_Major_Market'
            else:
                return f'{country_name}_Niche_Market'

        # Multiple small markets
        if avg_order_value > 150:
            return f'High_Value_Multi_Market_{segment_id}'
        elif total_customers > 200:
            return f'High_Volume_Multi_Market_{segment_id}'
        else:
            return f'Emerging_Markets_Group_{segment_id}'

    def _describe_geographic_segment_real(self, segment_countries):
        """
        GERÃ‡EK VERÄ° bazlÄ± geographic segment description
        """
        countries = segment_countries.index.tolist()

        return {
            'country_list': countries,
            'market_size': 'Large' if segment_countries['Unique_Customers'].sum() > 1000 else 'Medium' if
            segment_countries['Unique_Customers'].sum() > 200 else 'Small',
            'avg_customers_per_country': segment_countries['Unique_Customers'].mean(),
            'avg_order_value': segment_countries['Avg_Order_Value'].mean(),
            'price_range': f"Â£{segment_countries['Avg_Price'].min():.2f} - Â£{segment_countries['Avg_Price'].max():.2f}",
            'total_revenue': segment_countries['Total_Revenue'].sum(),
            'market_maturity': 'Mature' if segment_countries['Total_Orders'].mean() > 100 else 'Developing'
        }

    def _manual_geographic_segmentation(self, country_features):
        """
        Az Ã¼lke varsa manual segmentation
        """
        print("ğŸ¯ Using rule-based geographic segmentation...")

        geographic_segments = {}

        for idx, (country, data) in enumerate(country_features.iterrows()):
            segment_name = f"{country.replace(' ', '_')}_Market"

            geographic_segments[idx] = {
                'name': segment_name,
                'countries': [country],
                'country_count': 1,
                'avg_order_value': data['Avg_Order_Value'],
                'total_customers': data['Unique_Customers'],
                'total_revenue': data['Total_Revenue'],
                'characteristics': {
                    'country_list': [country],
                    'market_size': 'Large' if data['Unique_Customers'] > 1000 else 'Medium' if data[
                                                                                                   'Unique_Customers'] > 200 else 'Small',
                    'avg_order_value': data['Avg_Order_Value'],
                    'total_revenue': data['Total_Revenue']
                }
            }

            print(f"ğŸ—ºï¸ Segment {idx}: {segment_name}")
            print(f"   ğŸ‘¥ Customers: {data['Unique_Customers']:,}")
            print(f"   ğŸ’° Avg order: Â£{data['Avg_Order_Value']:.2f}")

        # Add to merged data
        country_to_segment = {country: idx for idx, country in enumerate(country_features.index)}
        self.merged_data['Auto_Geographic_Segment'] = self.merged_data['Country'].map(country_to_segment)

        return geographic_segments

    def _assign_behavior_pattern_name(self, pattern_customers, pattern_id):
        """
        GERÃ‡EK RFM segment distribution bazlÄ± behavior pattern naming
        """
        avg_spend = pattern_customers['Total_Spend'].mean()
        avg_frequency = pattern_customers['Order_Frequency'].mean()
        avg_recency = pattern_customers['Recency'].mean()

        # Analyze actual RFM segment distribution in this pattern
        segment_distribution = pattern_customers['Segment'].value_counts()
        dominant_segment = segment_distribution.index[0] if len(segment_distribution) > 0 else 'Unknown'
        segment_purity = segment_distribution.iloc[0] / len(pattern_customers) if len(pattern_customers) > 0 else 0

        # Data-driven naming based on actual patterns
        if dominant_segment == 'Champions' and segment_purity > 0.6:
            return 'Champions_Dominant_Pattern'
        elif dominant_segment == 'Loyal' and segment_purity > 0.5:
            return 'Loyal_Customer_Pattern'
        elif dominant_segment == 'At Risk' and segment_purity > 0.4:
            return 'At_Risk_Pattern'
        elif 'Champions' in segment_distribution.index and 'Loyal' in segment_distribution.index:
            return 'High_Value_Mixed_Pattern'
        elif avg_spend > 1000 and avg_frequency > 8:
            return 'Premium_Frequent_Buyers'
        elif avg_spend > 1000 and avg_frequency <= 3:
            return 'Premium_Occasional_Buyers'
        elif avg_frequency > 8 and avg_spend <= 500:
            return 'Frequent_Budget_Buyers'
        elif avg_recency > 180:
            return 'Long_Dormant_Pattern'
        elif avg_recency < 30:
            return 'Very_Active_Pattern'
        elif 'New Customers' in segment_distribution.index and segment_distribution['New Customers'] > 5:
            return 'New_Customer_Pattern'
        elif 'Hibernating' in segment_distribution.index and segment_distribution['Hibernating'] > 10:
            return 'Hibernating_Pattern'
        else:
            # Use dominant segment + spend level for naming
            spend_level = 'High' if avg_spend > 500 else 'Medium' if avg_spend > 200 else 'Low'
            return f'{dominant_segment}_{spend_level}_Spend_Pattern'

    def _get_category_name(self, category_id):
        """Get category name by ID"""
        if pd.isna(category_id) or category_id == -1:
            return 'Unknown_Category'
        return self.discovered_product_categories.get(int(category_id), {}).get('name', f'Category_{int(category_id)}')

    def _get_geographic_name(self, segment_id):
        """Get geographic segment name by ID"""
        if pd.isna(segment_id):
            return 'Unknown_Geographic'
        return self.discovered_geographic_segments.get(int(segment_id), {}).get('name',
                                                                                f'Geo_Segment_{int(segment_id)}')

    def _get_behavior_name(self, pattern_id):
        """Get behavior pattern name by ID"""
        if pd.isna(pattern_id):
            return 'Unknown_Behavior'
        return self.discovered_patterns['behavior_patterns'].get(int(pattern_id), {}).get('name',
                                                                                          f'Behavior_{int(pattern_id)}')

    def _categorize_price_tier(self, avg_price):
        """Categorize price tier"""
        if avg_price <= 2:
            return 0  # Budget
        elif avg_price <= 5:
            return 1  # Mid-tier
        elif avg_price <= 10:
            return 2  # Premium
        else:
            return 3  # Luxury

    def _calculate_segment_pattern_alignment(self, customer_data):
        """
        GERÃ‡EK VERÄ° bazlÄ± segment-pattern alignment calculation
        """
        # RFM segment vs discovered behavior pattern alignment
        rfm_segment = customer_data.get('Segment', 'Unknown')
        behavior_pattern_id = customer_data.get('Behavior_Pattern', -1)

        # Get the behavior pattern info
        if behavior_pattern_id != -1 and behavior_pattern_id in self.discovered_patterns.get('behavior_patterns', {}):
            pattern_info = self.discovered_patterns['behavior_patterns'][behavior_pattern_id]
            dominant_segments = pattern_info.get('dominant_segments', {})

            # Calculate alignment score based on how well this customer's segment
            # matches the dominant segments in their discovered pattern
            if rfm_segment in dominant_segments:
                segment_count_in_pattern = dominant_segments[rfm_segment]
                total_in_pattern = sum(dominant_segments.values())
                alignment_score = segment_count_in_pattern / total_in_pattern if total_in_pattern > 0 else 0
                return min(alignment_score * 2, 1.0)  # Scale to 0-1
            else:
                return 0.3  # Low alignment

        return 0.5  # Neutral alignment

    def _calculate_geographic_behavior_score(self, customer_data):
        """
        GERÃ‡EK VERÄ° bazlÄ± geographic behavior score calculation
        """
        # Geographic segment behavior consistency score
        total_spend = customer_data.get('Total_Spend', 0)
        avg_order = customer_data.get('Avg_Order', 0)
        order_frequency = customer_data.get('Order_Frequency', 0)
        geographic_segment_id = customer_data.get('Geographic_Segment', -1)

        if geographic_segment_id != -1 and geographic_segment_id in self.discovered_geographic_segments:
            geo_segment_info = self.discovered_geographic_segments[geographic_segment_id]

            # Compare customer behavior with segment averages
            segment_avg_order_value = geo_segment_info.get('avg_order_value', 0)

            # Calculate how well customer fits their geographic segment behavior
            if segment_avg_order_value > 0:
                order_value_similarity = 1 - abs(avg_order - segment_avg_order_value) / segment_avg_order_value
                order_value_similarity = max(0, min(1, order_value_similarity))

                # Combine with frequency and spend patterns
                spend_normalization = min(total_spend / 1000, 1.0)  # Normalize high spenders
                frequency_normalization = min(order_frequency / 10, 1.0)  # Normalize high frequency

                return (order_value_similarity * 0.5 + spend_normalization * 0.3 + frequency_normalization * 0.2)

        return 0.5  # Neutral score if no geographic data

    def _handle_enhanced_missing_values(self, df):
        """Handle missing values in enhanced dataset"""
        # Numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in ['Customer ID']:
                df[col].fillna(df[col].median(), inplace=True)

        # Categorical columns
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if col not in ['Customer ID']:
                df[col].fillna('Unknown', inplace=True)

        return df


def main():
    """
    Main ML Auto-Discovery Pipeline - GERÃ‡EK CSV VERÄ°SÄ° Ä°LE
    """
    print("ğŸš€ ML-POWERED AUTO SEGMENTATION ENGINE")
    print("ğŸ¤– Deep Learning ile GERÃ‡EK CSV Verinizden Otomatik Pattern Discovery")
    print("ğŸ“Š Placeholder veriler YOK - sadece YOUR ACTUAL DATA!")
    print("=" * 70)

    # Initialize ML engine
    ml_engine = MLAutoSegmentationEngine()

    # Run complete ML discovery on YOUR REAL DATA
    discovery_results = ml_engine.run_complete_ml_discovery()

    print(f"\nğŸ‰ GERÃ‡EK VERÄ° BAZLI ML AUTO-DISCOVERY COMPLETE!")
    print(f"ğŸ“Š Your actual country distribution analyzed and clustered")
    print(f"ğŸ·ï¸ Your actual product descriptions categorized automatically")
    print(f"ğŸ‘¤ Your actual customer behaviors discovered and patterned")
    print(f"ğŸ§  Enhanced features: 5 â†’ {len(discovery_results['neural_features'][2])} (real data based)")
    print(f"ğŸ¯ Ready for neural network integration with YOUR DATA!")

    return ml_engine, discovery_results


if __name__ == "__main__":
    ml_engine, results = main()